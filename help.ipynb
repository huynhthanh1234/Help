{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "name": "glove",
      "provenance": []
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "accelerator": "GPU"
  },
  "cells": [
    {
      "cell_type": "code",
      "metadata": {
        "id": "B-B5kJInAqKl",
        "colab_type": "code",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 124
        },
        "outputId": "e20051f7-39f3-4f0d-f729-f671c0ac9a0a"
      },
      "source": [
        "from google.colab import drive\n",
        "drive.mount(\"/content/gdrive\")"
      ],
      "execution_count": 1,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "Go to this URL in a browser: https://accounts.google.com/o/oauth2/auth?client_id=947318989803-6bn6qk8qdgf4n4g3pfee6491hc0brc4i.apps.googleusercontent.com&redirect_uri=urn%3aietf%3awg%3aoauth%3a2.0%3aoob&response_type=code&scope=email%20https%3a%2f%2fwww.googleapis.com%2fauth%2fdocs.test%20https%3a%2f%2fwww.googleapis.com%2fauth%2fdrive%20https%3a%2f%2fwww.googleapis.com%2fauth%2fdrive.photos.readonly%20https%3a%2f%2fwww.googleapis.com%2fauth%2fpeopleapi.readonly\n",
            "\n",
            "Enter your authorization code:\n",
            "··········\n",
            "Mounted at /content/gdrive\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "yDbvwksUfG-3",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "from sklearn.feature_extraction.text import CountVectorizer\n",
        "from sklearn.feature_extraction.text import TfidfTransformer\n",
        "from sklearn.feature_extraction.text import TfidfVectorizer\n",
        "import numpy as np\n",
        "import pandas as pd\n",
        "from sklearn.feature_extraction.text import CountVectorizer\n",
        "import pickle"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "kGAFXR2kfc7S",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "import json\n",
        "import pandas as pd\n",
        "data_train = [json.loads(line) for line in open('gdrive/My Drive/springer-train.jsonl', 'r')]\n",
        "df_train=pd.DataFrame(data_train)\n",
        "df1=pd.concat([df_train.abstract,df_train.title,df_train.keywords,df_train.journal],axis=1)\n",
        "\n",
        "data_valid = [json.loads(line) for line in open('gdrive/My Drive/springer-valid.jsonl', 'r')]\n",
        "df_valid=pd.DataFrame(data_valid)\n",
        "df2=pd.concat([df_valid.abstract,df_valid.title,df_valid.keywords,df_valid.journal],axis=1)\n",
        "\n",
        "df_new_train=df1.dropna(subset=[\"abstract\"])\n",
        "df_new_valid=df2.dropna(subset=[\"abstract\"])\n"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "aezZT_klfe_H",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "df1=pd.DataFrame({'Abstract': list(df_new_train.abstract),\n",
        "                  'Title': list(df_new_train.title),\n",
        "                  'Key': list(df_new_train.keywords),\n",
        "                  'category': list(df_new_train.journal)})\n",
        "df2=pd.DataFrame({'Abstract': list(df_new_valid.abstract),\n",
        "                  'Title': list(df_new_valid.title),\n",
        "                  'Key': list(df_new_valid.keywords),\n",
        "                  'category': list(df_new_valid.journal)})"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "SYVqFlsnf1AA",
        "colab_type": "code",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 69
        },
        "outputId": "50c3498b-d0fe-42e1-96c5-58f9bd0bea4f"
      },
      "source": [
        "import re\n",
        "import string\n",
        "import nltk\n",
        "\n",
        "# Keras\n",
        "from keras.preprocessing.text import Tokenizer\n",
        "from keras.preprocessing.sequence import pad_sequences\n",
        "from keras.models import Sequential\n",
        "from keras.layers import Dense, Flatten, LSTM, Conv1D, MaxPooling1D, Dropout, Activation\n",
        "from keras.layers.embeddings import Embedding\n",
        "\n",
        "## Plot\n",
        "import plotly.offline as py\n",
        "import plotly.graph_objs as go\n",
        "py.init_notebook_mode(connected=True)\n",
        "import matplotlib as plt\n",
        "\n",
        "# NLTK\n",
        "import nltk\n",
        "from nltk.corpus import stopwords\n",
        "from nltk.stem import PorterStemmer\n",
        "\n",
        "# Other\n",
        "import re\n",
        "import string\n",
        "import numpy as np\n",
        "import pandas as pd\n",
        "from sklearn.manifold import TSNE\n",
        "import nltk\n",
        "nltk.download('stopwords')\n",
        "def preprocess_abstract(text):\n",
        "    \n",
        "\n",
        "    text =re.sub(\"\\$\\$.*?\\$\\$\", \"\", text)\n",
        "    text =re.sub(r\"(https.*)\", \"\", text)\n",
        "    text =re.sub(\"\\$.*?\\$\", \"\", text)\n",
        "    text =re.sub(\"\\\\\\\\\\(.*?\\\\\\\\\\)\", \"\", text)\n",
        "    text =re.sub(\"\\\\\\\\\\[.*?\\\\\\\\\\]\", \"\", text)\n",
        "    text =re.sub(\"\\[.*?\\]\", \"\", text)\n",
        "    text =re.sub(\"{.*?}\", \"\", text)\n",
        "    text =re.sub(r\"\\\\begin.*?\\\\end\", \"\", text)\n",
        "    text = re.sub(\"[^\\w\\s\\d{}]d\".format(string.punctuation), \"\", text)\n",
        "    text = text.lower()\n",
        "    text = re.sub('[^a-z \\-]',\"\",text)\n",
        "    text=' '.join([item for item in  re.sub('[^a-z \\n\\-]', ' ', text).split(' ') if len(item) >= 2])\n",
        "    \n",
        "    ## Convert words to lower case and split them\n",
        "    text = text.split()\n",
        "    \n",
        "    ## Remove stop words\n",
        "    stops = set(stopwords.words(\"english\"))\n",
        "    text = [w for w in text if not w in stops and len(w) >= 3]\n",
        "    stemmer = PorterStemmer()\n",
        "    stemmed_words = [stemmer.stem(word) for word in text]\n",
        "    text = \" \".join(stemmed_words)\n",
        "    return text\n",
        "\n",
        "for i in range(0,len(df1.Abstract)):\n",
        "  df1.Abstract[i]=preprocess_abstract(df1.Abstract[i])\n",
        "for i in range(0,len(df2.Abstract)):\n",
        "  df2.Abstract[i]=preprocess_abstract(df2.Abstract[i])"
      ],
      "execution_count": 5,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "Using TensorFlow backend.\n"
          ],
          "name": "stderr"
        },
        {
          "output_type": "display_data",
          "data": {
            "text/html": [
              "        <script type=\"text/javascript\">\n",
              "        window.PlotlyConfig = {MathJaxConfig: 'local'};\n",
              "        if (window.MathJax) {MathJax.Hub.Config({SVG: {font: \"STIX-Web\"}});}\n",
              "        if (typeof require !== 'undefined') {\n",
              "        require.undef(\"plotly\");\n",
              "        requirejs.config({\n",
              "            paths: {\n",
              "                'plotly': ['https://cdn.plot.ly/plotly-latest.min']\n",
              "            }\n",
              "        });\n",
              "        require(['plotly'], function(Plotly) {\n",
              "            window._Plotly = Plotly;\n",
              "        });\n",
              "        }\n",
              "        </script>\n",
              "        "
            ]
          },
          "metadata": {
            "tags": []
          }
        },
        {
          "output_type": "stream",
          "text": [
            "[nltk_data] Downloading package stopwords to /root/nltk_data...\n",
            "[nltk_data]   Unzipping corpora/stopwords.zip.\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "8lgisLYnf7_O",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "vocabulary_size = 20000\n",
        "tokenizer = Tokenizer(num_words= vocabulary_size)\n",
        "tokenizer.fit_on_texts(df1.Abstract)\n",
        "\n",
        "sequences = tokenizer.texts_to_sequences(df1.Abstract)\n",
        "data = pad_sequences(sequences, maxlen=300)"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "3iv66oUdqvbX",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "vocabulary_size = 20000\n",
        "tokenizer = Tokenizer(num_words= vocabulary_size)\n",
        "tokenizer.fit_on_texts(df2.Abstract)\n",
        "\n",
        "sequences = tokenizer.texts_to_sequences(df2.Abstract)\n",
        "data_valid = pad_sequences(sequences, maxlen=300)"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "PEl9v4Yj152v",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        ""
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "zcuynzlNsQyf",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "train_y=df1.category\n",
        "valid_y=df2.category\n",
        "from sklearn.preprocessing import OneHotEncoder\n",
        "onehot_encoder = OneHotEncoder(sparse=False)\n",
        "train_y = onehot_encoder.fit_transform(train_y.to_numpy().reshape(len(train_y), 1))\n",
        "valid_y = onehot_encoder.transform(valid_y.to_numpy().reshape(len(valid_y), 1))"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "7PV6DRmMo2p6",
        "colab_type": "code",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 35
        },
        "outputId": "dcc941b3-ef7e-448c-be0c-23e06d959fd0"
      },
      "source": [
        "embeddings_index = dict()\n",
        "f = open('gdrive/My Drive/glove.6B.100d.txt')\n",
        "for line in f:\n",
        "    values = line.split()\n",
        "    word = values[0]\n",
        "    coefs = np.asarray(values[1:], dtype='float32')\n",
        "    embeddings_index[word] = coefs\n",
        "f.close()\n",
        "print('Loaded %s word vectors.' % len(embeddings_index))"
      ],
      "execution_count": 10,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "Loaded 400000 word vectors.\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "Ff6GDmn4p1gh",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "# create a weight matrix for words in training docs\n",
        "embedding_matrix = np.zeros((vocabulary_size, 100))\n",
        "for word, index in tokenizer.word_index.items():\n",
        "    if index > vocabulary_size - 1:\n",
        "        break\n",
        "    else:\n",
        "        embedding_vector = embeddings_index.get(word)\n",
        "        if embedding_vector is not None:\n",
        "            embedding_matrix[index] = embedding_vector"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "te3IwVarp5kg",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "model_glove = Sequential()\n",
        "model_glove.add(Embedding(vocabulary_size, 100, input_length=300, weights=[embedding_matrix], trainable=False))\n",
        "model_glove.add(Dropout(0.2))\n",
        "model_glove.add(Conv1D(64, 5, activation='relu'))\n",
        "model_glove.add(MaxPooling1D(pool_size=4))\n",
        "model_glove.add(LSTM(100))\n",
        "model_glove.add(Dense(196, activation='sigmoid'))\n",
        "model_glove.compile(loss='binary_crossentropy', optimizer='adam', metrics=['accuracy'])\n"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "cNj9frFIp-jo",
        "colab_type": "code",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 69
        },
        "outputId": "16c9a82a-7b08-4728-8aa5-854e285e4397"
      },
      "source": [
        "model_glove.fit(data, np.array(train_y), validation_data=(data_valid,np.array(valid_y)) , epochs = 2)"
      ],
      "execution_count": 0,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "Train on 244740 samples, validate on 26323 samples\n",
            "Epoch 1/2\n",
            " 61728/244740 [======>.......................] - ETA: 12:32 - loss: 0.0352 - accuracy: 0.9934"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "f-A_JpU-qUz_",
        "colab_type": "code",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 35
        },
        "outputId": "417fe245-f035-47ea-bd90-817684308719"
      },
      "source": [
        "print(len(df1.category))"
      ],
      "execution_count": 16,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "244740\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "2bE7lyf5qXx1",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        ""
      ],
      "execution_count": 0,
      "outputs": []
    }
  ]
}